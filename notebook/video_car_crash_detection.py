# -*- coding: utf-8 -*-
"""video-car-crash-detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S1_3iVSHer1l0_N8mAAaB023IhuO_F2P
"""

!pip install pytube
!pip install stonesoup
!pip install ffmpeg
!pip install object_detection
!pip install  moviepy

"""## copy to the path
/usr/local/lib/python3.9/dist-packages/object_detection/protos/string_int_label_map_pb2.py

Video processing, Vehicle Crash detection & Tracking
==============================================
"""

# get the video file

import os
from pytube import YouTube
#VIDEO_FILENAME = 'sample1'
#VIDEO_FILENAME = '1-accident-lower'
VIDEO_FILENAME = '2-disabled-car-lower'
VIDEO_EXTENTION = '.mp4'
VIDEO_PATH = os.path.join(os.getcwd(), VIDEO_FILENAME+VIDEO_EXTENTION)

# VideoClipReader
# ***************

import datetime
from stonesoup.reader.video import VideoClipReader
start_time = datetime.timedelta(minutes=0, seconds=10)
end_time = datetime.timedelta(minutes=0, seconds=12)
frame_reader = VideoClipReader(VIDEO_PATH, start_time, end_time)

from moviepy.video.fx import all
frame_reader.clip = all.crop(frame_reader.clip, 100, 100)
num_frames = len(list(frame_reader.clip.iter_frames()))

print('Number of frames: ', num_frames)

# FFmpegVideoStreamReader

from copy import copy
from PIL import Image
from matplotlib import pyplot as plt
from matplotlib import animation
from IPython.display import HTML

fig, ax = plt.subplots(num="VideoClipReader output")
artists = []

print('Running FrameReader example...')
for timestamp, frame in frame_reader:
    if not (len(artists)+1) % 10:
        print("Frame: {}/{}".format(len(artists)+1, num_frames))

    # Read the frame pixels
    pixels = copy(frame.pixels)

    # Plot output
    image = Image.fromarray(pixels)
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    fig.tight_layout()
    artist = ax.imshow(image, animated=True)
    artists.append([artist])

ani = animation.ArtistAnimation(fig, artists, interval=20, blit=True, repeat_delay=200)
# HTML(ani.to_html5_video())

# TensorFlowBoxObjectDetector

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)
import pathlib
import tensorflow as tf
tf.gfile = tf.io.gfile

tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)

# Enable GPU dynamic memory allocation
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Download and extract model
def download_model(model_name):
    base_url = 'http://download.tensorflow.org/models/object_detection/'
    model_file = model_name + '.tar.gz'
    model_dir = tf.keras.utils.get_file(fname=model_name,
                                        origin=base_url + model_file,
                                        untar=True)
    model_dir = pathlib.Path(model_dir)/"saved_model"
    return str(model_dir)

MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco_2018_01_28'
PATH_TO_MODEL = download_model(MODEL_NAME)

# Download labels file
def download_labels(filename):
    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'
    label_dir = tf.keras.utils.get_file(fname=filename,
                                        origin=base_url + filename,
                                        untar=False)
    label_dir = pathlib.Path(label_dir)
    return str(label_dir)

LABEL_FILENAME = 'mscoco_label_map.pbtxt'
PATH_TO_LABELS = download_labels(LABEL_FILENAME)

# Building the detector

from stonesoup.detector.tensorflow import TensorFlowBoxObjectDetector

run_async = False                           # Configure the detector to run in synchronous mode
detector = TensorFlowBoxObjectDetector(frame_reader, PATH_TO_MODEL, PATH_TO_LABELS,
                                       run_async=run_async)

# %%
# Filtering-out unwanted detections
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# In this section we showcase how we can utilise Stone Soup :class:`~.Feeder` objects in order to
# filter out unwanted detections. One example of feeder we can use is the
# :class:`~.MetadataValueFilter`, which allows us to filter detections by applying a custom
# operator on particular fields of the :attr:`~.Detection.metadata` property of detections.
#
# Each detection generated by :class:`~.TensorFlowBoxObjectDetector` carries the following
# :attr:`~.Detection.metadata` fields:
#
#  - ``raw_box``: The raw bounding box containing the normalised coordinates ``[y_0, x_0, y_1, x_1]``, as generated by TensorFlow.
#  - ``class``: A dict with keys ``id`` and ``name`` relating to the id and name of the detection class.
#  - ``score``: A float in the range ``(0, 1]`` indicating the detector's confidence.
#
# Detection models trained on the MSCOCO dataset, such as the one we downloaded, are able to detect
# 90 different classes of objects (see the `downloaded .pbtxt file <https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt>`_
# for a full list). Instead, as we discussed at the beginning of the tutorial, we wish to limit the
# detections to only those classified as cars. This can be done as follows:
from stonesoup.feeder.filter import MetadataValueFilter
detector = MetadataValueFilter(detector, 'class', lambda x: x['name'] == 'car')

# %%
# Continuing, we may want to filter out detections which have a low confidence score:
detector = MetadataValueFilter(detector, 'score', lambda x: x > 0.1)

# %%
# Finally, we observed that the detector tends to incorrectly generate detections which are much
# larger the the size we expect for a car. Therefore, we can filter out those detections by only
# allowing ones whose width is less the 20\% of the frame width (i.e. ``x_1-x_0 < 0.2``):
detector = MetadataValueFilter(detector, 'raw_box', lambda x: x[3]-x[1] < 0.2)

# %%
# You are encouraged to comment out any/all of the above filter definitions and observe the
# produced output.

class RectClass(object):
    def __init__(self, x0, y0, x1, y1):
        self.x0 = x0
        self.y0 = x0
        self.x1 = x1
        self.y1 = y1

def overlappingObjects(r1x0, r1y0, r1x1, r1y1, r2x0, r2y0, r2x1, r2y1):
    return not (r1x0 < r2x0 
                or r1x1 > r2x1 
                or r1y0 < r2y0 
                or r1y1 > r2y1)

# Reading and visualising detections

import numpy as np
from PIL import ImageDraw

def draw_detections(timestamp, image, detections, show_class=False, show_score=False):
    """ Draw detections on an image

    Parameters
    ----------
    image: :class:`PIL.Image`
        Image on which to draw the detections
    detections: : set of :class:`~.Detection`
        A set of detections generated by :class:`~.TensorFlowBoxObjectDetector`
    show_class: bool
        Whether to draw the class of the object. Default is ``False``
    show_score: bool
        Whether to draw the score of the object. Default is ``False``

    Returns
    -------
    : :class:`PIL.Image`
        Image with detections drawn
    """
    objDetected = []
    draw = ImageDraw.Draw(image)
    for detection in detections:
        x0, y0, w, h = np.array(detection.state_vector).reshape(4)
        x1, y1 = (x0 + w, y0 + h)
        print(timestamp, x0, y0, x1, y1)
        objDetected.append(RectClass(x0, y0, x1, y1))

        draw.rectangle([x0, y0, x1, y1], outline=(0, 255, 0), width=1)
        class_ = detection.metadata['class']['name']
        score = round(float(detection.metadata['score']),2)
        if show_class and show_score:
            draw.text((x0,y1 + 2), '{}:{}'.format(class_, score), fill=(0, 255, 0))
        elif show_class:
            draw.text((x0, y1 + 2), '{}'.format(class_), fill=(0, 255, 0))
        elif show_score:
            draw.text((x0, y1 + 2), '{}'.format(score), fill=(0, 255, 0))

    print('----')
    crashed = False
    for rect1 in objDetected:
        for rect2 in objDetected:
          if rect1.x0 != rect2.x0 and rect1.y0 != rect2.y0:
            crashed = overlappingObjects(rect1.x0, rect1.y0, rect1.x1, rect1.y1,
                                       rect2.x0, rect2.y0, rect2.x1, rect2.y1)
    print('Crashed: ', crashed)

    if not crashed:
      del draw
      
    return image

fig2, ax2 = plt.subplots(num="TensorFlowBoxObjectDetector output")
artists2 = []
print("Running TensorFlowBoxObjectDetector example... Be patient...")
for timestamp, detections in detector:
    if not (len(artists2)+1) % 10:
        print("Frame: {}/{}".format(len(artists2)+1, num_frames))

    # Read the frame pixels
    frame = frame_reader.frame
    pixels = copy(frame.pixels)

    # Plot output
    image = Image.fromarray(pixels)
    image = draw_detections(timestamp, image, detections, True, True)
    ax2.axes.xaxis.set_visible(False)
    ax2.axes.yaxis.set_visible(False)
    fig2.tight_layout()
    artist = ax2.imshow(image, animated=True)
    artists2.append([artist])

ani2 = animation.ArtistAnimation(fig2, artists2, interval=20, blit=True, repeat_delay=200)
HTML(ani2.to_html5_video())